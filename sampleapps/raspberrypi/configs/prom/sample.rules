
# ========= RAM, JVM-Memory ==============

container_memory_percent=container_memory_usage_bytes / container_spec_memory_limit_bytes
jvm_memory_bytes_percent=jvm_memory_bytes_used / jvm_memory_bytes_max

# ========= CPU ==========================

#throttled seconds, per second considering the last 5 minutes (1=full stop)
container_cpu_throttle_seconds_rate=rate(container_cpu_cfs_throttled_seconds_total[5m])
#if throttled more than 10% of the time then we have a hotspot (especially if over some time)
container_cpu_throttle_hotspot=container_cpu_throttle_seconds_rate >bool 0.1
# how many are have a throttle-hotspot (can be grouped in influx+grafana)
container_cpu_throttle_hotspot_percent=sum(container_cpu_throttle_hotspot) / count(container_cpu_cfs_throttled_seconds_total)
# what percentage of its spec periods does a task get
container_cpu_period_percent=container_cpu_cfs_periods_total / container_spec_cpu_period

# ======== Disk ==========================

#container disk storage predicted in 4 hours
container_fs_bytes_left_in4h=container_fs_limit_bytes - predict_linear(container_fs_usage_bytes[1h], 60*60*4)

# ======== Network =======================

# we set limits for the network speed. a definition like this can be used 
# either as upper or lower limit. The enforcement of the limit is up to the
# provider of course, we just want to monitor if the network-bandwith
# we ordered fits our needs. When we set our limit just shy below the
# actual limit as ordered from a provider we can 

# another example
#		consider we have a metrics about the expected data rate at which we produce 
#		data. we can calculate a metrics as to how far we achieve the specified throughput 
#		for the slice e.g. we ordered 10 MB/s, our sensors produce 8 MB/s but we only
#		achieve 5 MB/s. Of course this can have multiple reasons but network-congestion
#		could be one of them. 
slice_network_limit_bytespersecond{cl_cd_stack_namespace="cloudstack"} = 100000.0
slice_network_limit_bytespersecond{cl_cd_stack_namespace="nfvstack"} = 10000.0
slice_network_limit_bytespersecond{cl_cd_stack_namespace="iotstack"} = 1000.0

container_network_transmit_bytes_sumtotal=sum(container_network_transmit_bytes_total) by (cl_cd_stack_namespace, cl_cd_swarm_service_name)
container_network_transmit_bytes_percent=rate(container_network_transmit_bytes_sumtotal[2m])  / ignoring(cl_cd_swarm_service_name)  group_left(cl_cd_stack_namespace) slice_network_limit_bytespersecond

container_network_receive_bytes_sumtotal=sum(container_network_receive_bytes_total) by (cl_cd_stack_namespace, cl_cd_swarm_service_name)
container_network_receive_bytes_percent=rate(container_network_receive_bytes_sumtotal[2m])  / ignoring(cl_cd_swarm_service_name)  group_left(cl_cd_stack_namespace) slice_network_limit_bytespersecond

# ======= Availability ===============

# it would of course also be possible to use the up-metric for jobs

# we need to round otherwise there can be values like 0.99999993434 because 
#	same with clamp_max. 
# internally double is used. 
# this gives us the availability on an _instance level_
# we could add a parameter for the maximum "latency" of the container_last_seen
# value so we achieve 1.0 even if the response takes a little time (e.g. 100 ms). 
instance_availability_2min=clamp_max(rate(container_last_seen[2m]), 1.0)
instance_availability_5min=clamp_max(rate(container_last_seen[5m]), 1.0)
instance_availability_15min=clamp_max(rate(container_last_seen[15m]), 1.0)
instance_availability_60min=clamp_max(rate(container_last_seen[60m]), 1.0)

# on the _sub-slice level_ we must consider that some services may
# be run in replicated mode, so there is more than one instance and
# as long as 1 instance (or maybe a specified number of instances)
# is in service it is available
service_availability_2m=sum(instance_availability_2min) by (cl_cd_swarm_service_name, cl_cd_stack_namespace)
service_availability_5m=sum(instance_availability_5min) by (cl_cd_swarm_service_name, cl_cd_stack_namespace)
service_availability_15m=sum(instance_availability_15min) by (cl_cd_swarm_service_name, cl_cd_stack_namespace)
service_availability_60m=sum(instance_availability_60min) by (cl_cd_swarm_service_name, cl_cd_stack_namespace)

# finally we compare the sla with the availability
# so we get 0.0s for services which are down
service_slaavailability_2m=clamp_max(service_availability_2m /  ignoring(cl_cd_stack_namespace) group_right(cl_cd_swarm_service_name) sla_service_up, 1) or (sla_service_up-sla_service_up)
service_slaavailability_5m=clamp_max(service_availability_5m /  ignoring(cl_cd_stack_namespace) group_right(cl_cd_swarm_service_name) sla_service_up, 1) or (sla_service_up-sla_service_up)
service_slaavailability_15m=clamp_max(service_availability_15m /  ignoring(cl_cd_stack_namespace) group_right(cl_cd_swarm_service_name) sla_service_up, 1) or (sla_service_up-sla_service_up)
service_slaavailability_60m=clamp_max(service_availability_60m /  ignoring(cl_cd_stack_namespace) group_right(cl_cd_swarm_service_name) sla_service_up, 1) or (sla_service_up-sla_service_up)

# on the _sub-slice_ we can group by the stack (or env) and use min() to determine the health
# same on the _full slice_ ... hm actually i'm not sure if that makes lots of sense
# other options: <bool 0.95 to see if any SLAs are violated   
service_slaavailability_2m_fullfilled=service_slaavailability_2m >bool 0.99
service_slaavailability_5m_fullfilled=service_slaavailability_5m >bool 0.99
service_slaavailability_15m_fullfilled=service_slaavailability_15m >bool 0.99
service_slaavailability_60m_fullfilled=service_slaavailability_60m >bool 0.99

# ======= Mean Time Between Failure or Mean Time to Repair =======

# determine gaps in container-last-seen
# so we have the values 1,2,3,4 ,4,4,4,4, 9,10,11 so there is a gap for 5-8 (MTTR=3 seconds)
# the rate for this is < 1; theres a change/increase function
# remember that the metric dissapears after a while. i need to store the
# last value somehow or modify the filter-proxy to retain (think mqtt-retain)
# the container-last seen value longer than cadvisor provides them
# hm maybe count/count_scalar(container_last_seen) - changes(container_last_seen) 
# that would give us the gap-size


# === Application Metrics ===
#The following example expression returns the difference in CPU temperature between now and 2 hours ago
#delta(cpu_temp_celsius{host="zeus"}[2h])

# = Alerts =

ALERT container_missing
IF instance_availability_2min < 0.9
FOR 40s
LABELS { severity = "warn" }
  ANNOTATIONS {
    summary = "Instance {{ $labels.instance }} down",
    description = "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 40 seconds.",
  }

ALERT job_missing
if up < 1
FOR 80s
LABELS { severity = "warn" }
  ANNOTATIONS {
    summary = "Instance {{ $labels.instance }} down",
    description = "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 40 seconds.",
  }
  
ALERT sla_violated
if service_slaavailability_5m_fullfilled < 0.9
FOR 80s
LABELS { severity = "warn" }
  ANNOTATIONS {
    summary = "Service  {{ $labels.cl_cd_swarm_service_name }} violates sla",
    description = "The services {{ $labels.cl_cd_swarm_service_name }} that violates the sla",
  }  