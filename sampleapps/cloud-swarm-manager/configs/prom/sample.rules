my_container_spec_cpus = container_spec_cpu_quota / 100000
my_container_cpu_percent = (sum(rate(container_cpu_user_seconds_total[1m])) by (cl_cd_swarm_service_name, cl_cd_swarm_task_name)) / ignoring(cl_cd_swarm_service_name, cl_cd_swarm_task_name) group_left count(node_cpu{mode="system"})  * 100
# ========= RAM, JVM-Memory ==============

container_memory_percent=container_memory_usage_bytes / container_spec_memory_limit_bytes
jvm_memory_bytes_percent=jvm_memory_bytes_used / jvm_memory_bytes_max

# ========= CPU ==========================

#throttled seconds, per second considering the last 5 minutes (1=full stop)
container_cpu_throttle_seconds_rate=rate(container_cpu_cfs_throttled_seconds_total[5m])
#if throttled more than 10% of the time then we have a hotspot (especially if over some time)
container_cpu_throttle_hotspot=container_cpu_throttle_seconds_rate >bool 0.1
# how many are have a throttle-hotspot (can be grouped in influx+grafana)
container_cpu_throttle_hotspot_percent=sum(container_cpu_throttle_hotspot) / count(container_cpu_cfs_throttled_seconds_total)
# what percentage of its spec periods does a task get
container_cpu_period_percent=container_cpu_cfs_periods_total / container_spec_cpu_period

# ======== Disk ==========================

warn_container_fs_bytes_left_prediction_minutes = 60
container_fs_bytes_left_inWarnLimitMins=container_fs_limit_bytes - predict_linear(container_fs_usage_bytes[1h], 60*scalar(warn_container_fs_bytes_left_prediction_minutes))

#container disk storage predicted in 4 hours
container_fs_bytes_left_in4h=container_fs_limit_bytes - predict_linear(container_fs_usage_bytes[1h], 60*60*4)

# ======== Network =======================

# we set limits for the network speed. a definition like this can be used 
# either as upper or lower limit. The enforcement of the limit is up to the
# provider of course, we just want to monitor if the network-bandwith
# we ordered fits our needs. When we set our limit just shy below the
# actual limit as ordered from a provider we can 

# another example
#		consider we have a metrics about the expected data rate at which we produce 
#		data. we can calculate a metrics as to how far we achieve the specified throughput 
#		for the slice e.g. we ordered 10 MB/s, our sensors produce 8 MB/s but we only
#		achieve 5 MB/s. Of course this can have multiple reasons but network-congestion
#		could be one of them. 
slice_network_limit_bytespersecond{cl_cd_stack_namespace="cloudstack"} = 100000.0
slice_network_limit_bytespersecond{cl_cd_stack_namespace="nfvstack"} = 10000.0
slice_network_limit_bytespersecond{cl_cd_stack_namespace="iotstack"} = 1000.0

container_network_transmit_bytes_sumtotal=sum(container_network_transmit_bytes_total) by (cl_cd_stack_namespace, cl_cd_swarm_service_name)
container_network_transmit_bytes_percent=rate(container_network_transmit_bytes_sumtotal[2m])  / ignoring(cl_cd_swarm_service_name)  group_left(cl_cd_stack_namespace) slice_network_limit_bytespersecond

container_network_receive_bytes_sumtotal=sum(container_network_receive_bytes_total) by (cl_cd_stack_namespace, cl_cd_swarm_service_name)
container_network_receive_bytes_percent=rate(container_network_receive_bytes_sumtotal[2m])  / ignoring(cl_cd_swarm_service_name)  group_left(cl_cd_stack_namespace) slice_network_limit_bytespersecond

# ======= Availability ===============


service_up = sum(up) by (job) or (target_service_up * 0)
sla_service_up = service_up / target_service_up 

# FALLBACK if we do not have up metrics because the service does not have 
# a metric endpoint: 
# 	we consider the container up if the difference between last seen and now 
# 	is smaller than a max response-time (we cloud make that even configurable -> scalar(x)). 
container_up = (time() - container_last_seen) <bool 100
servicecontainer_up=sum(container_up) by (cl_cd_swarm_service_name) or (target_servicecontainer * 0)
sla_servicecontainer_up = servicecontainer_up / target_servicecontainers


# ======= Mean Time Between Failure or Mean Time to Repair =======

# determine gaps in container-last-seen
# so we have the values 1,2,3,4 ,4,4,4,4, 9,10,11 so there is a gap for 5-8 (MTTR=3 seconds)
# the rate for this is < 1; theres a change/increase function
# remember that the metric dissapears after a while. i need to store the
# last value somehow or modify the filter-proxy to retain (think mqtt-retain)
# the container-last seen value longer than cadvisor provides them
# hm maybe count/count_scalar(container_last_seen) - changes(container_last_seen) 
# that would give us the gap-size


# === Application Metrics ===
#The following example expression returns the difference in CPU temperature between now and 2 hours ago
#delta(cpu_temp_celsius{host="zeus"}[2h])

# = Alerts =

ALERT container_missing
IF instance_availability_2min < 0.9
FOR 40s
LABELS { severity = "warn" }
  ANNOTATIONS {
    summary = "Instance {{ $labels.instance }} down",
    description = "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 40 seconds.",
  }

ALERT job_missing
if up < 1
FOR 80s
LABELS { severity = "warn" }
  ANNOTATIONS {
    summary = "Instance {{ $labels.instance }} down",
    description = "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 40 seconds.",
  }
  
ALERT sla_violated
if service_slaavailability_5m_fullfilled < 0.9
FOR 80s
LABELS { severity = "warn" }
  ANNOTATIONS {
    summary = "Service  {{ $labels.cl_cd_swarm_service_name }} violates sla",
    description = "The services {{ $labels.cl_cd_swarm_service_name }} that violates the sla",
  }  